{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMA forcing files\n",
    "We need to convert the standard ERA5 forcing files (dimensions: lon, lat, time), into SUMMA-ready forcing files (dimensions: time, hru). \n",
    "\n",
    "Note: SUMMA expects the forcing file dimensions in the order (time, hru). See: https://summa.readthedocs.io/en/latest/input_output/SUMMA_input/#meteorological-forcing-files. Fortran and Python interact with netcdf dimensions in different ways and therefore the dimensions in this bit of Python code must be specified as (hru, time). This will ensure that Fortran interprets these in its desired order.\n",
    "\n",
    "Explanation: \n",
    "This is a result of how multi-dimensional arrays are stored into memory in both languages. Fortran is `column-major`, whereas Python's `numpy` (which underlies the `netCDF4` library) is `row-major`. Note that this is completely **unrelated** to the way the multi-dimensional array is indexed, which is typically (row,column) in any programming language. See: https://en.wikipedia.org/wiki/Row-_and_column-major_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import sys # to handle command line arguments (sys.argv[0] = name of this file, sys.argv[1] = arg1, ...)\n",
    "import netCDF4 as nc4\n",
    "import numpy as np\n",
    "from pathlib import Path # Load the path module to avoid windows/linux issues\n",
    "#from simpledbf import Dbf5 # used to create a dataframe from .dbf file (Graham has issues with geopandas)\n",
    "import geopandas as gpd # replace simpledbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General values\n",
    "year = 1979\n",
    "month = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations\n",
    "path_att = Path('C:/Git_repos/summaWorkflow/2_MS_experiment_setup/NA_catchmentMeritHydro_ERA5_SOILGRIDS_MODISVEG_Graham/1_create_attributes/')\n",
    "file_att = path_att / 'attributes_na_catchment_initial.nc' # for HRU order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the intersection shapefile, that tells us which bits of ERA5 to cut out for use with the NLDAS grid\n",
    "path_intersection_shp = Path('C:/Git_repos/summaWorkflow/2_MS_experiment_setup/NA_catchmentMeritHydro_ERA5_SOILGRIDS_MODISVEG_Graham/4_create_forcing/intersect_output_cleaned')\n",
    "file_intersection_shp = path_intersection_shp / 'intersect_era5_forcing_and_merit_hydro_basins.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify ERA5 source forcing file path and name\n",
    "path_source = Path('C:/Globus endpoint')\n",
    "name_source = 'ERA5_NA_' + str(year) + str(month).zfill(2) + '.nc'\n",
    "file_source = path_source / name_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify target forcing file path and name\n",
    "path_dest = Path('C:/Globus endpoint')\n",
    "name_dest = 'ERA5_NA_gridEra5_SUMMA_' + str(year) + str(month).zfill(2) + '.nc'\n",
    "file_dest = path_dest / name_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HRU ID ordering in attributes.nc file\n",
    "# ---------------------------------------------------------------\n",
    "with nc4.Dataset(file_att) as att:\n",
    "    IDs_att = att.variables['hruId'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the intersection shapefile and convert into a dataframe\n",
    "# ---------------------------------------------------------------\n",
    "shp_intersection = gpd.read_file(file_intersection_shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CANDEX approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant CANDEX code\n",
    "# CANDEX modules\n",
    "# ---------------------------------------------------------------\n",
    "import glob\n",
    "import time\n",
    "#import geopandas as gpd\n",
    "import netCDF4 as nc4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from shapely.geometry import Polygon\n",
    "# not neccessary for the function but for visualziation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# additional packages for speed-up\n",
    "import shapefile # PyShp library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the CANDEX case variable, indicating that source has a regular lat lon grid\n",
    "case = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimension and variable names in the source file\n",
    "time_dim = 'time' # name of the dimension time\n",
    "lat_dim = 'latitude' # name of the dimension lat\n",
    "lon_dim = 'longitude' # name of the dimension lon\n",
    "lat_var_name = 'latitude' # name of the variable lat\n",
    "lon_var_name = 'longitude' # name of the variable lon\n",
    "\n",
    "# Define what we want to transfer straight from the source .nc file\n",
    "transfer_var = ['airpres','LWRadAtm','SWRadAtm','pptrate','airtemp','spechum','windspd']\n",
    "transfer_att = ['missing_value','units','long_name','standard_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'C:\\\\Globus endpoint\\\\ERA5_NA_197901.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ea0b5ba18c68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mnc4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_source\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Read the time data in the source file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# check the step size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtime_unit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;31m# units of the step size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'C:\\\\Globus endpoint\\\\ERA5_NA_197901.nc'"
     ]
    }
   ],
   "source": [
    "with nc4.Dataset(file_source) as src:\n",
    "    \n",
    "    # Read the time data in the source file\n",
    "    time_step = src.variables['time'][1:2] - src.variables['time'][0:1] # check the step size\n",
    "    time_unit = src.variables['time'].units # units of the step size\n",
    "        \n",
    "    # We need the data step size in [s] for SUMMA\n",
    "    # Find the conversion factor that takes [time_unit] to [s]\n",
    "    if \"second\" in time_unit:\n",
    "        time_conversion_factor = 1 # 1 second in a second\n",
    "    elif \"minute\" in time_unit:\n",
    "        time_conversion_factor = 60 # 60 seconds in a minute\n",
    "    elif \"hour\" in time_unit:\n",
    "        time_conversion_factor = 3600 # etc.\n",
    "    elif \"day\" in time_unit:\n",
    "        time_conversion_factor = 86400\n",
    "    else:\n",
    "        time_conversion_factor = -1 # fill value to indicate we couldn't find the time step size\n",
    "        \n",
    "    # Calculate the data_step [s]\n",
    "    data_step = time_step[0] * time_conversion_factor # time_step is a list. use [0] to ensure data_step doesn't become a list\n",
    "    print('Time step size is '+ str(time_step) + ' with units \\'' + time_unit + '\\'. data_step = ' + str(data_step) + 's.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CANDEX functions\n",
    "# netcdf reads\n",
    "def read_value_lat_lon_nc(case,\n",
    "                          lat_target, lon_target, name_of_nc,\n",
    "                          name_of_variable, name_of_time_dim,\n",
    "                          name_of_lat_var, name_of_lon_var,\n",
    "                          name_of_lat_dim, name_of_lon_dim):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/candex\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  Apache2\n",
    "\n",
    "    This function funcitons read different grids and sum them up based on the\n",
    "    weight provided to aggregate them over a larger area\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    case: value [1,]\n",
    "            1 is for 3-dimensional variable with 1-dimentional lat and lon\n",
    "            2 is for 3-dimensional varibale with 2-dimentional lat and lon\n",
    "            3 is for 2-dimensional variable with 1-dimentional lat and lon (time series)\n",
    "    lat_target: lat value [1,]\n",
    "    lon_target: lon value [1,]\n",
    "    name_of_nc: full or part of nc file(s) name including nc, string, example 'XXX/*01*.nc'\n",
    "    name_of_variable: name of the varibale, string\n",
    "    name_of_time_dim: name of time dimension, string\n",
    "    name_of_lat_var: name of lat variable, string\n",
    "    name_of_lon_var: name of lon variable, string\n",
    "    name_of_lat_dim: name of lat dimension, string\n",
    "    name_of_lon_dim: name of lon dimension, string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: a numpy array that has the read value of the NetCDF file for the lats, lons and weights\n",
    "    \"\"\"\n",
    "    names_all = glob.glob(name_of_nc)\n",
    "    names_all.sort()\n",
    "    data = None\n",
    "\n",
    "    # for to read on variouse nc files for target lat lon\n",
    "    for names in names_all:\n",
    "        \n",
    "        da = xr.open_dataset(names, decode_times=False)\n",
    "        \n",
    "        # case 1, the varibale is 3-dimensional and lat and lon are one-dimnesional\n",
    "        if case ==1:\n",
    "            # finding the index for the lat for target_lat\n",
    "            da_lat = da[name_of_lat_var] # reading the lat variable\n",
    "            temp = np.array(abs(da_lat-lat_target)) # finding the distance to target_lat\n",
    "            index_target_lat = np.array([temp.argmin()]) # finding the closest index to target_lat\n",
    "            \n",
    "            # finding the index for the lon for target_lon\n",
    "            da_lon = da[name_of_lon_var] # reading the lon variable\n",
    "            temp = np.array(abs(da_lon-lon_target)) # finding the distnace to target_lon\n",
    "            index_target_lon = np.array([temp.argmin()]) # finding the closest index to target_lon\n",
    "            \n",
    "            # making sure that the lat and lon are only one value and not two\n",
    "            index_target_lon = index_target_lon[0]\n",
    "            index_target_lat = index_target_lat[0]\n",
    "            \n",
    "#             # porder of dimensions for the target variable\n",
    "#             dataset = da[name_of_variable]\n",
    "#             order_time_dim = dataset.dims.index(name_of_time_dim)\n",
    "#             order_lat_dim = dataset.dims.index(name_of_lat_dim)\n",
    "#             order_lon_dim = dataset.dims.index(name_of_lon_dim)\n",
    "            \n",
    "#             if order_time_dim == 0: # such as the varibaele dimension is time, lat, lon\n",
    "#                 data_temp = dataset [:,index_target_lat,index_target_lon]\n",
    "#             if order_time_dim == 2: # such as the varibaele dimension is lon, lat, time\n",
    "#                 data_temp = dataset [index_target_lon,index_target_lat,:]\n",
    "                \n",
    "            \n",
    "            # porder of dimensions for the target variable\n",
    "            order_time_dim = da.variables[name_of_variable].dims.index(name_of_time_dim)\n",
    "            order_lat_dim = da.variables[name_of_variable].dims.index(name_of_lat_dim)\n",
    "            order_lon_dim = da.variables[name_of_variable].dims.index(name_of_lon_dim)\n",
    "            \n",
    "            if order_time_dim == 0: # such as the varibaele dimension is time, lat, lon\n",
    "                data_temp = da[name_of_variable][:,index_target_lat,index_target_lon]\n",
    "            if order_time_dim == 2: # such as the varibaele dimension is lon, lat, time\n",
    "                data_temp = da[name_of_variable][index_target_lon,index_target_lat,:]\n",
    "\n",
    "        # case 2, the varibale is 3-dimensional and lat and lon are 2-dimentional such as rotated lat lon\n",
    "        if case ==2:\n",
    "            # finding the index for the lat\n",
    "            da_lat = da[name_of_lat_var]\n",
    "            da_lon = da[name_of_lon_var]\n",
    "            temp = np.array(abs(da_lat-lat_target)+abs(da_lon-lon_target))\n",
    "            ind = np.unravel_index(np.argmin(temp, axis=None), temp.shape)\n",
    "            ind = np.array(ind)\n",
    "            \n",
    "            # order of dimensions for the target variable\n",
    "            dataset = da[name_of_variable]\n",
    "            order_time_dim = dataset.dims.index(name_of_time_dim)\n",
    "            order_lat_dim = dataset.dims.index(name_of_lat_dim)\n",
    "            order_lon_dim = dataset.dims.index(name_of_lon_dim)\n",
    "            \n",
    "            if order_time_dim == 0: # such as the varibaele dimension is time, lat, lon\n",
    "                index_target_lat = ind[0]\n",
    "                index_target_lon = ind[1]\n",
    "                data_temp = dataset [:,index_target_lat,index_target_lon]\n",
    "            if order_time_dim == 2: # such as the varibaele dimension is lon, lat, time\n",
    "                index_target_lat = ind[1]\n",
    "                index_target_lon = ind[0]\n",
    "                data_temp = dataset [index_target_lon,index_target_lat,:]\n",
    "\n",
    "        # case 3, the varibale is 2-dimnesional and lat and lon are 1-dimensional such as n time or time n\n",
    "        if case ==3:\n",
    "            da_lat = da[name_of_lat_var]\n",
    "            da_lon = da[name_of_lon_var]\n",
    "            temp = np.array(abs(da_lat-lat_target)+abs(da_lon-lon_target))\n",
    "            ind = np.unravel_index(np.argmin(temp, axis=None), temp.shape)\n",
    "            ind = np.array(ind)\n",
    "            ind = ind[0]\n",
    "            \n",
    "            # order of dimensions for the target variable\n",
    "            dataset = da[name_of_variable]\n",
    "            order_time_dim = dataset.dims.index(name_of_time_dim)\n",
    "            \n",
    "            if order_time_dim == 0: # such as the varibaele dimension is time, n\n",
    "                index_target_n = ind\n",
    "                data_temp = dataset [:,index_target_n]\n",
    "            if order_time_dim == 1: # such as the varibaele dimension is n, time\n",
    "                index_target_n = ind\n",
    "                data_temp = dataset [index_target_n,:]\n",
    "        \n",
    "        # getting the length of time dimension\n",
    "        time_steps = da.dims[name_of_time_dim]\n",
    "        \n",
    "        # put the read data into the data_temp\n",
    "        data_temp = np.array(data_temp)\n",
    "        data_temp = data_temp.reshape((time_steps, ))\n",
    "\n",
    "        # append the data_temp\n",
    "        if data is not None:\n",
    "            data = np.append(data, data_temp)\n",
    "        else:\n",
    "            data = data_temp\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "# area weighting\n",
    "def area_ave(case,\n",
    "             lat, lon, w,\n",
    "             name_of_nc, name_of_variable,\n",
    "             name_of_time_dim,\n",
    "             name_of_lat_dim, name_of_lon_dim,\n",
    "             name_of_lat_var, name_of_lon_var):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/candex\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  Apache2\n",
    "\n",
    "    This function funcitons read different grids and sum them up based on the\n",
    "    weight provided to aggregate them over a larger area\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    case: value [1,]\n",
    "            1 is for 3-dimensional variable with 1-dimentional lat and lon\n",
    "            2 is for 3-dimensional varibale with 2-dimentional lat and lon\n",
    "            3 is for 2-dimensional variable with 1-dimentional lat and lon (time series)\n",
    "    lat: lat value [1,]\n",
    "    lon: lon value [1,]\n",
    "    w: wieght[1,]\n",
    "    name_of_nc: full or part of nc file(s) name including nc, string, example 'XXX/*01*.nc'\n",
    "    name_of_variable: name of the varibale, string\n",
    "    name_of_time_dim: name of time dimension, string\n",
    "    name_of_lat_dim: name of lat dimension, string\n",
    "    name_of_lon_dim: name of lon dimension, string\n",
    "    name_of_lat_var: name of lat variable, string\n",
    "    name_of_lon_var: name of lon variable, string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: a numpy array that has the read value of the NetCDF file for the lats, lons and weights\n",
    "    \"\"\"\n",
    "    \n",
    "    data = None \n",
    "    #print(w, lat, lon)\n",
    "    if lat.size ==1: # only one entry to the funciton (one lat, one lon and one W)\n",
    "        data_temp = read_value_lat_lon_nc(case,\n",
    "                                          lat, lon, name_of_nc,\n",
    "                                          name_of_variable, name_of_time_dim,\n",
    "                                          name_of_lat_dim, name_of_lon_dim,\n",
    "                                          name_of_lat_var, name_of_lon_var)\n",
    "        data = data_temp * w\n",
    "    else:\n",
    "        for i in np.arange(lat.shape[0]):# itterate over target values\n",
    "            data_temp = read_value_lat_lon_nc(case,\n",
    "                                              lat[i], lon[i], name_of_nc,\n",
    "                                              name_of_variable, name_of_time_dim,\n",
    "                                              name_of_lat_dim, name_of_lon_dim,\n",
    "                                              name_of_lat_var, name_of_lon_var)\n",
    "            if i == 0: # multiply the read value with their weight and sum\n",
    "                data = data_temp * w[i]\n",
    "            else:\n",
    "                data = data + data_temp * w[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Create the new netcdf file\n",
    "with nc4.Dataset(file_dest, \"w\", format=\"NETCDF4\") as dest, nc4.Dataset(file_source) as src:\n",
    "    \n",
    "    # === Some general attributes\n",
    "    dest.setncattr('Author', \"Created by W. Knoben from ERA5 source data\")\n",
    "    dest.setncattr('History','Created ' + time.ctime(time.time()))\n",
    "    dest.setncattr('Source','Written using a script from the library of Wouter Knoben (Github link)')\n",
    "    dest.setncattr('Reason','Takes (1) ERA5 data in lat/lon format for the North America domain, (2) a prepared shape file of the intersection between ERA5 grid and the NLDAS grid that we want to use, and creates SUMMA-ready (HRU-based) forcing.')\n",
    "    \n",
    "    # === Meta attributes from source\n",
    "    for name in src.ncattrs():\n",
    "        dest.setncattr(name, src.getncattr(name))\n",
    "    \n",
    "    # === Define the dimensions\n",
    "    dest.createDimension('hru',len(IDs_att)) # use the number of HRU's defined in the attribute file\n",
    "    dest.createDimension('time',len(src.dimensions['time'])) # source must have a 'time' dimension. Copy length of this dimension\n",
    "    \n",
    "    # === Define the HRU-specific topograhic variables: hruId, latitude, longitude\n",
    "    # These values come from CANDEX. hruID is needed as one of the SUMMA dimensions (the other dimension is time).\n",
    "    # lat/lon are the centers of the polygon in the case of gridded data. These are not used by SUMMA but kept in\n",
    "    # case we want to use them to plot things later.\n",
    "    var = 'hruId'\n",
    "    dest.createVariable(var, 'i4', 'hru', fill_value = False)\n",
    "    dest[var].setncattr('long_name', 'ID of the Hydrologic Response Unit')\n",
    "    \n",
    "    var = 'lat'\n",
    "    dest.createVariable(var, 'f4', 'hru', fill_value = False)\n",
    "    dest[var].setncattr('long_name', 'latitude')\n",
    "    dest[var].setncattr('units', 'degrees_north')    \n",
    "    \n",
    "    var = 'lon'\n",
    "    dest.createVariable(var, 'f4', 'hru', fill_value = False)\n",
    "    dest[var].setncattr('long_name', 'longitude')\n",
    "    dest[var].setncattr('units', 'degrees_east')    \n",
    "    \n",
    "    # === Create the non-HRU-specific variables: time and data_step\n",
    "    var = 'data_step'\n",
    "    dest.createVariable(var, 'f4', (), fill_value = False)\n",
    "    dest[var].setncattr('long_name', 'Length of time step')\n",
    "    dest[var].setncattr('units','seconds')\n",
    "    dest.variables[var][:] = data_step\n",
    "    \n",
    "    var = 'time'\n",
    "    dest.createVariable(var, src.variables[var].datatype, src.variables[var].dimensions, fill_value = -999)\n",
    "    dest[var].setncatts(src.variables[var].__dict__)\n",
    "    dest.variables[var][:] = src.variables[var][:]\n",
    "    \n",
    "    # === Define the HRU-specific topograhic variables: pptrate, LWRadAtm, SWRadAtm, airpres, airtemp, spechum, windspd\n",
    "    # Copy these directly from the source file to ensure data provenance\n",
    "    for name, variable in src.variables.items():\n",
    "        \n",
    "        # Transfer the forcing variables using the pre-defined list, using dimensions 'hru' and 'time'\n",
    "        if name in transfer_var:\n",
    "            dest.createVariable(name, variable.datatype, ('hru','time'), fill_value = -999)\n",
    "            dest[name].setncatts(src[name].__dict__)\n",
    "    \n",
    "    # === Get the HRU ID's from the shapefile 'shp' on path 'path_shp'\n",
    "    IDs = shp_intersection['hruId'].to_numpy().astype('int')\n",
    "    \n",
    "    # === Initialize a progress report\n",
    "    # progress = 0\n",
    "    \n",
    "    # === Loop over HRU's and fill the variables\n",
    "    for i in IDs_att:\n",
    "        \n",
    "        # Show a progress report\n",
    "        # if (progress%100) == 0:\n",
    "        #    print(str(progress) + ' out of ' + str(len(IDs)) + ' HRUs completed.')\n",
    "        \n",
    "        # We need to ensure that each HRU ID goes into the destination file in the right order:\n",
    "        # Find the index of this particular HRU ID in attributes.nc\n",
    "        idx = np.where(IDs_att == i)\n",
    "        idx = list(idx[0]) # convert numpy array to list that we can use as index\n",
    "        \n",
    "        # Read the appropriate row(s) from the shapefile\n",
    "        row = shp_intersection.loc[IDs == i]\n",
    "        \n",
    "        # Store topographic variables in the .nc file \n",
    "        # 'idx' reflects their position in the 'hru' dimension\n",
    "        dest['hruId'][idx] = np.array(row['hruId'].iloc[0]) \n",
    "        #dest['lat'][idx] = np.array(row['hrulat'].iloc[0]) \n",
    "        #dest['lon'][idx] = np.array(row['hrulon'].iloc[0]) \n",
    "        \n",
    "        # Read the lat, lon and weights\n",
    "        lat = np.array(row.latForce) # lat of source\n",
    "        lon = np.array(row.lonForce) # lon of source\n",
    "        W = np.array(row.WForce) # contribution of source in targesh shape\n",
    "        \n",
    "        # Loop over forcing variables defined in 'transfer_var'\n",
    "        for var in transfer_var:\n",
    "            \n",
    "            # Calculate the area weighted forcing\n",
    "            # Note that the Pathlib path 'file_source' must be forced into a string for CANDEX to work\n",
    "            awf = area_ave(case, lat, lon, W, str(file_source), var,\\\n",
    "                           time_dim, lat_dim, lon_dim, lat_var_name, lon_var_name )\n",
    "            \n",
    "            # Store this forcing variable in the .nc file \n",
    "            # 'idx' reflects its position in the 'hru' dimension\n",
    "            dest[var][idx,:] = awf\n",
    "            \n",
    "# timer\n",
    "end = time.time()\n",
    "print('execution took' + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andy Wood's approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compAvgVal(wgt, i_index, j_index, overlaps, f_dataIn, varname):\n",
    "    \"\"\"Compute areal weighted avg value of <varname> in <nc_in> for all output polygons\n",
    "       based on input/output overlap weights in <nc_wgt>\"\"\"\n",
    "\n",
    "    # now read the input timeseries file\n",
    "    print(\"-------------------\")\n",
    "    print(\"reading input timeseries data for %s \" % varname)\n",
    "    dataVals = f_dataIn.variables[varname][:]    \n",
    "    print(\"INFO: value at [0,0,0]: %f\" % (dataVals[0,0,0]))\n",
    "\n",
    "    matDataVals = np.zeros((nOutPolys, maxOverlaps))\n",
    "    wgtedVals   = np.zeros((nTimeSteps, nOutPolys))\n",
    "\n",
    "    # format data into shape matching weights [nOutPolys, maxOverlaps]\n",
    "    for t in range(0, nTimeSteps):\n",
    "\n",
    "        # format data into regular matrix matching format of weights (nOutPolygons, maxOverlaps)\n",
    "        #   used advanced indexing to extract matching input grid indices\n",
    "        for p in range(start_poly, (end_poly+1)):\n",
    "            matDataVals[p, 0:overlaps[p]] = \\\n",
    "                dataVals[t, [j_index[overlapStartNdx[p]:overlapEndNdx[p]]], [i_index[overlapStartNdx[p]:overlapEndNdx[p]]] ] \n",
    "    \n",
    "        wgtedVals[t, ] = np.sum(matDataVals * matWgts, axis=1)   # produces vector of weighted values\n",
    "        print(\" averaged var %s timestep %d\" % (varname, t))\n",
    "\n",
    "    return wgtedVals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geospatialTools]",
   "language": "python",
   "name": "conda-env-geospatialTools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
